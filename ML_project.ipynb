{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning project using Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " O objetivo dessa lista de exercícios é desenvolver um projeto de Machine Learning usando uma base já conhecida na literatura. O título do exercício deve ser \"Um exercício de Machine Learning usando a base XXX do Kaggle.\" Espera-se que cada base seja usada por no máximo um estudante. Logo, se você decidir fazer um desses exercícios, explicite na planilha que base você pretende estudar.\n",
    "\n",
    "Considere que você está enfrentando um projeto real de machine learning. Então, sugere-se que, para resolver esse exercício, você considere os seguintes passos abaixo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Defina o seu problema (objetivo): O que você está tentando prever? Que tipo de problema você tem? Supervisionado? Não-supervisionado?\n",
    "\n",
    "**2)** Colete os dados: Quais são as variáveis disponíveis? Estatísticas descritivas? As variáveis são contínuas? Categóricas? Binárias? Possuem variáveis multicolineares? Existem dados faltantes (*missing data*)? Como lidar com missing data? Você precisará enriquecer sua base?  Você precisará normalizar a sua base de dados? Histogramas de variáveis (suas variáveis possuem dinâmica)? Correlação entre variáveis?\n",
    "\n",
    "**3)** Divida os dados em treino/teste: Sua base é balanceada? Você precisa pensar em alguma estratégia especial para dividir a sua base? Você considera estratégias como *K-fold validation*?\n",
    "\n",
    "**4)** Construa o seu *benchmark*. Sempre comece com modelo bem simples que você tenha controle. Uma boa escolha é um modelo linear em que  você consiga interpretar os seus resultados/ter uma ideia das variáveis que podem influenciar os seus resultados.\n",
    "\n",
    "**5)** Você precisará de um procedimento específico para escolha dos hiperparâmetros?\n",
    "\n",
    "**6)** Você precisará de um procedimento específico de seleção de variáveis?\n",
    "\n",
    "**7)** Você precisará regularizar o seu modelo? Como você pretende regularizá-los?\n",
    "\n",
    "**8)** Modelos industriais são suficientes (modelos de regressão, árvores de decisão, *SVM*)?\n",
    "\n",
    "**9)** Você precisará trabalhar com redes neurais rasas ou profundas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para ajudar a sua solução, você pode buscar ideias em soluções prévias disponíveis na literatura/internet, mas o código deve ser desenvolvido por você. Finalizando, o conteúdo usado para solucionar essas questões é todo o material do curso e não dessa aula. Para resolver essa questão, você pode usar qualquer base de dados disponível em: https://www.kaggle.com/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Define your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I shall predict some movies box office. In order to do so, I'll use [The Movie Database's (TMDB) dataset](https://www.kaggle.com/c/tmdb-box-office-prediction/overviews), containing information like the movies' cast, release data, their original language, runtime and genra. Since I already have the revenue for the train dataset, it will be a *supervised* problem; therefore, some possible algorithms are linear regression models, support vector regressions (SVR), decision tree regressions and random forest regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Collect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames and Arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Other\n",
    "import zipfile # To deal with zips\n",
    "import ast # Parsing dictionary variables\n",
    "import requests # Web scraping\n",
    "import time # For the requests' sleep\n",
    "from collections import Counter # Counts occurrences in dictionaries\n",
    "import cpi # Inflation adjustments\n",
    "from datetime import date\n",
    "\n",
    "# Guarantees code will accurately inflate to today's dollar\n",
    "cpi.update()\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets\n",
    "zf = zipfile.ZipFile('datasets/tmdb-box-office-prediction.zip') \n",
    "train = pd.read_csv(zf.open('train.csv'))\n",
    "test = pd.read_csv(zf.open('test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns that are JSON objects\n",
    "dict_cols = ['genres', 'production_companies', 'production_countries', 'cast', 'crew']\n",
    "\n",
    "def text_to_dict(df):\n",
    "    \"\"\" Transforms JSON columns from strings to dictionaries \"\"\"\n",
    "    for col in dict_cols:\n",
    "        df[col] = df[col].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n",
    "    return df\n",
    "\n",
    "for df in [train, test]:\n",
    "    df = text_to_dict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of the train dataset: \", train.shape)\n",
    "print(\"Shape of the test dataset: \", test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the test dataset is larger than the train set, which is unusual. Also, the test dataset does not contain any information on the target variable (the movies' revenue). Let's take a look first at the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A brief look at the data\n",
    "train.head() # First five observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 23 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   id                     3000 non-null   int64  \n",
      " 1   belongs_to_collection  604 non-null    object \n",
      " 2   budget                 3000 non-null   int64  \n",
      " 3   genres                 3000 non-null   object \n",
      " 4   homepage               946 non-null    object \n",
      " 5   imdb_id                3000 non-null   object \n",
      " 6   original_language      3000 non-null   object \n",
      " 7   original_title         3000 non-null   object \n",
      " 8   overview               2992 non-null   object \n",
      " 9   popularity             3000 non-null   float64\n",
      " 10  poster_path            2999 non-null   object \n",
      " 11  production_companies   3000 non-null   object \n",
      " 12  production_countries   3000 non-null   object \n",
      " 13  release_date           3000 non-null   object \n",
      " 14  runtime                2998 non-null   float64\n",
      " 15  spoken_languages       2980 non-null   object \n",
      " 16  status                 3000 non-null   object \n",
      " 17  tagline                2403 non-null   object \n",
      " 18  title                  3000 non-null   object \n",
      " 19  Keywords               2724 non-null   object \n",
      " 20  cast                   3000 non-null   object \n",
      " 21  crew                   3000 non-null   object \n",
      " 22  revenue                3000 non-null   int64  \n",
      "dtypes: float64(2), int64(3), object(18)\n",
      "memory usage: 539.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info() # Variable types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that *release_date* variable was compiled as an \"object\" type (ie, probably a *string* variable), instead of *datetime*. All other variables are assigned to their correct types. It will be corrected further on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Test's revenues and merging it to Train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to increase the number of observations, I will scrap the revenue values of the movies from 'test' dataset using the TMDB's Application Programming Interface (API) and, subsequently, merge both train and test datasets into a single \"complete\" dataset. The train-test split will then be done later on, using this complete dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TMDB's API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TMDB's API is a free interface which only requires you to create an account in order to use it. Once you're all set, you can perfom \"GET\" requests using, for instance, the Python's \"request\" package. The request will return a JSON object, which is nothing more than a string object containing dictionaries of elements. The JSON object can then be parsed (interpreted) thereafter using requests' \".json()\" method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the movies' TMDB ids are required to obtain the movies' revenues, and they are not directly available from our datasets. However, it is possible to return them perfoming requests using the TMDB API's \"find\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_TMDB_id(imdb_id, api_key):\n",
    "    \"\"\" Returns the tmdb_id for a given imdb_id, using TMDB API's \"find\" method\n",
    "    \n",
    "    imdb_id: self-explanatory\n",
    "    api_key: your TMDB API key.\n",
    "    \"\"\"\n",
    "    url = \"https://api.themoviedb.org/3/find/\" + imdb_id\n",
    "\n",
    "    # The parameters to be used on the request\n",
    "    querystring = {\"api_key\": api_key,\n",
    "                   \"language\":\"en-US\",\n",
    "                   \"external_source\":\"imdb_id\"}\n",
    "    \n",
    "    # Making the request and parsing its text from JSON format to Python's dictionaries\n",
    "    response = requests.request(\"GET\", url, params=querystring)\n",
    "    parsed_response = response.json()\n",
    "    \n",
    "    # Getting the tmdb_id\n",
    "    tmdb_id = parsed_response['movie_results'][0]['id']\n",
    "    \n",
    "    return str(tmdb_id)\n",
    "\n",
    "\n",
    "def get_revenue(tmdb_id, api_key):\n",
    "    \"\"\" Given the movie's tmdb_id, returns its revenue, using TMDB API's \"movie\" method\n",
    "    \n",
    "    tmdb_id: self-explanatory\n",
    "    api_key: your TMDB API key\n",
    "    \"\"\"\n",
    "    url = \"https://api.themoviedb.org/3/movie/\" + tmdb_id\n",
    "\n",
    "    querystring = {\"api_key\": api_key}\n",
    "\n",
    "    # Getting the request object\n",
    "    response = requests.request(\"GET\", url, params=querystring)\n",
    "\n",
    "    # Parsing the information we want\n",
    "    parsed_response = response.json()\n",
    "    revenue = parsed_response['revenue']\n",
    "    \n",
    "    return revenue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make the requests. It is worth noting that you will need your own API key in order to do so. Also, it is highly recommended to do a limited number of requests *per minute* to avoid overloading the system and, hence, being banned from it. I'll limit them to 20 *per minute*. \n",
    "\n",
    "Finally, note also that this procedure will take approximately 7 hours to be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating lists to receive the imdb_ids and also the revenues\n",
    "test_imdb_ids = [x for x in test['imdb_id']]\n",
    "the_revenues = [0] * len(test_imdb_ids)\n",
    "# api_key = \"\" # YOUR API_KEY HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# WARNING: the following code chunk requires a TMDB API key (which you can get for free on their website)\\n# It also takes about 7 hours to run. If you __REALLY__ want to run it, replace \"if False:\" on the following\\n# line of code to \"if True:\"\\n\\nif False:\\n    for i in range(len(test_imdb_ids)):\\n        # Fill the values\\n        print(i)\\n        try:\\n            next_tmdb_id = get_TMDB_id(test_imdb_ids[i], api_key)\\n            the_revenues[i] = get_revenue(next_tmdb_id, api_key)\\n        except:\\n            print(\"An error occurred on observation \", i)\\n\\n        # Avoids excessive requests\\n        # After 20 requests, stops the execution for one minute\\n        # (remember: each loop counts as 2 requests)\\n        if (i + 1) % 10 == 0:\\n            print(\"Step: \", i)\\n            time.sleep(60)\\n\\n    print(\"\\nDone!\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# WARNING: the following code chunk requires a TMDB API key (which you can get for free on their website)\n",
    "# It also takes about 7 hours to run. If you __REALLY__ want to run it, replace \"if False:\" on the following\n",
    "# line of code to \"if True:\"\n",
    "\n",
    "if False:\n",
    "    for i in range(len(test_imdb_ids)):\n",
    "        # Fill the values\n",
    "        print(i)\n",
    "        try:\n",
    "            next_tmdb_id = get_TMDB_id(test_imdb_ids[i], api_key)\n",
    "            the_revenues[i] = get_revenue(next_tmdb_id, api_key)\n",
    "        except:\n",
    "            print(\"An error occurred on observation \", i)\n",
    "\n",
    "        # Avoids excessive requests\n",
    "        # After 20 requests, stops the execution for one minute\n",
    "        # (remember: each loop counts as 2 requests)\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(\"Step: \", i)\n",
    "            time.sleep(60)\n",
    "\n",
    "    print(\"\\nDone!\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the revenue values to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(the_revenues).to_excel('test_revenues.xlsx', header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['revenue'] = pd.read_excel('datasets/test_revenues.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the movies for which the information on the revenue was not available, and merging the dataframes thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test.drop(test[test.revenue == 0].index) # drops those movies for which the revenue was not available\n",
    "\n",
    "# Merging the dataframes into a single df.\n",
    "frames = [train, test]\n",
    "complete_df = pd.concat(frames)\n",
    "\n",
    "# Removes missing data from \"budget\" variable\n",
    "complete_df = complete_df.loc[complete_df['budget'] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is a total of 7345 observations in the complete dataframe, about 2000 of them have budget value equal to zero, which can be interpreted as missing data. Indeed, I have tried both inputting the mean/median value and also dropping this missing data, and the latter perfomed way better in all metrics on cross-validation. Thus, I opted to drop this missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Date variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that the competition was supposed to conclude May 30, 2019. Therefore, all movies with year greater than 19 are from the last century, consequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_year(x):\n",
    "    \"\"\" Returns the year from the date.\n",
    "    \n",
    "    PS: the release date was originally a STRING on the \"mm/dd/yy\" format\n",
    "    \"\"\"\n",
    "    year = x.split('/')[2]\n",
    "    year = int(year)\n",
    "    return year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the year variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the YEAR variable\n",
    "complete_df['year'] = 0\n",
    "complete_df['year'] = complete_df['release_date'].apply(lambda x: gen_year(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute frequency of movies with release dates between 1920 and 1930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25    5\n",
       "28    4\n",
       "27    2\n",
       "26    2\n",
       "30    1\n",
       "29    1\n",
       "24    1\n",
       "22    1\n",
       "21    1\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the occurrences of years between 1920 and 1930\n",
    "print(\"Absolute frequency of movies with release dates between 1920 and 1930\")\n",
    "complete_df.loc[(complete_df['year'] <= 30) & (complete_df['year'] >= 20)]['year'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As there are almost no movies in the dataset from 1930 or older, it is reasonable to label all movies with year ending in 19 or below as 21th century movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_year_release_date(release_date):\n",
    "    \"\"\" Adds 1900 or 2000 to the 'release_date' variable's year\"\"\"\n",
    "    year = release_date.split('/')[2] # Picks the year\n",
    "    \n",
    "    # Corrects the year\n",
    "    if int(year) <= 19:\n",
    "        return release_date[:-2] + '20' + year\n",
    "    else:\n",
    "        return release_date[:-2] + '19' + year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects the 'year' column\n",
    "complete_df['year'] = complete_df['year'].apply(lambda x: 1900 + x if x > 19 else 2000 + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Release date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrects the 'release_date' column\n",
    "complete_df['release_date'] = complete_df['release_date'].apply(lambda x: fix_year_release_date(x))\n",
    "complete_df['release_date'] = pd.to_datetime(complete_df['release_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quarter and weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried also using release months instead of release quarters, but there was almost no performance improvements on cross-validation. Thus, I opted for the quarters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['release_quarter'] = complete_df['release_date'].dt.quarter\n",
    "complete_df['release_weekday'] = complete_df['release_date'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4\n",
       "1    4\n",
       "2    4\n",
       "3    4\n",
       "5    3\n",
       "Name: release_weekday, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Monday == 0, Friday == 4, Sunday == 6\n",
    "complete_df['release_weekday'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2015-02-20\n",
       "1   2004-08-06\n",
       "2   2014-10-10\n",
       "3   2012-03-09\n",
       "5   1987-08-06\n",
       "Name: release_date, dtype: datetime64[ns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df['release_date'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Revenue and Budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the values for inflation and creating the natural log version of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most recent movie release date\n",
    "max_date = complete_df['release_date'].max()\n",
    "\n",
    "# Inflation adjusting\n",
    "complete_df['revenue'] = complete_df.apply(lambda x: cpi.inflate(value = x.revenue, year_or_month = x.release_date, to = max_date), axis = 1)\n",
    "complete_df['budget'] = complete_df.apply(lambda x: cpi.inflate(value = x.budget, year_or_month = x.release_date, to = max_date), axis = 1)\n",
    "\n",
    "# log1p(x) = ln(x+1): it avoids calculating log(0), which is undefined\n",
    "complete_df['ln_revenue'] = complete_df['revenue'].apply(lambda x: np.log1p(x))\n",
    "complete_df['ln_budget'] = complete_df['budget'].apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As popularity has a skewed distribution, we shall create log popularity. I've also tested using (popularity, popularity^2, popularity^3 and popularity^4) against log popularity, and the latter perfomed better in all metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['ln_popularity'] = complete_df['popularity'].apply(lambda x: np.log1p(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are about 15 movies without their runtime information, or with 0 (zero) minutes runtime, which is almost the same, since it is virtuallyy impossible for a movie to be less than one minute long. However, all of this can be easily fixed by searching on IMDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding out the movies with NaN (missing data) on runtime variable\n",
    "complete_df['runtime'] = complete_df['runtime'].replace(0.0, np.nan) # Replacing 0 with NaN\n",
    "complete_df.loc[complete_df['runtime'] != complete_df['runtime'], ['id', 'title', 'runtime', 'imdb_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the runtime's missing values with IMDB's information\n",
    "complete_df.loc[complete_df['id'] == 1336,'runtime'] = 130 # Korolyov\n",
    "complete_df.loc[complete_df['id'] == 3244,'runtime'] = 93 # La caliente niña Julietta\n",
    "complete_df.loc[complete_df['id'] == 4490,'runtime'] = 91 # Pancho, el perro millonario\n",
    "complete_df.loc[complete_df['id'] == 4633,'runtime'] = 100 # Nunca en horas de clase\n",
    "complete_df.loc[complete_df['id'] == 6818,'runtime'] = 90 # Miesten välisiä keskusteluja\n",
    "complete_df.loc[complete_df['id'] == 391,'runtime'] = 96 # The Worst Christmas of My Life\n",
    "complete_df.loc[complete_df['id'] == 978,'runtime'] = 93 # La peggior settimana della mia vita\n",
    "complete_df.loc[complete_df['id'] == 1542,'runtime'] = 93 # All at Once\n",
    "complete_df.loc[complete_df['id'] == 2151,'runtime'] = 108 # Mechenosets\n",
    "complete_df.loc[complete_df['id'] == 2499,'runtime'] = 86 # Hooked on the Game 2. The Next Level\n",
    "complete_df.loc[complete_df['id'] == 2866,'runtime'] = 96 # Tutto tutto niente niente\n",
    "complete_df.loc[complete_df['id'] == 4074,'runtime'] = 103 # Shikshanachya Aaicha Gho\n",
    "complete_df.loc[complete_df['id'] == 4431,'runtime'] = 96 # Plus one\n",
    "complete_df.loc[complete_df['id'] == 5520,'runtime'] = 86 # Glukhar v kino\n",
    "complete_df.loc[complete_df['id'] == 5849,'runtime'] = 140 # Shabd\n",
    "complete_df.loc[complete_df['id'] == 6210,'runtime'] = 104 # The Last Breath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a \"budget/runtime\" ratio variable\n",
    "complete_df['budget_runtime_ratio'] = complete_df.budget / complete_df.runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variable I created was \"budget to runtime ratio\". I tested also the implementation of a \"squared runtime\" variable, but it resulted in little performance improvement to the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crew and cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the crew and cast, we'll create a variable for each with their size, and also a dummy variable \"top_50\", which takes 1 if the movie contains at least one of the top 50 most recurrent cast/crew member in the dataset, and 0 elsewise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class json_variables(object):\n",
    "    \"\"\" Handles JSON (ie, dictionary) variables. \"\"\"\n",
    "    \n",
    "    def __init__(self, df, variable, top_number):\n",
    "        \"\"\" Initiates the class.\n",
    "        \n",
    "        df: dataframe.\n",
    "        variable: the variable of interest (cast, crew, prod.companies, prod. countries or genre.)\n",
    "        top_number: threshold. Example: top_number = 30 means it will consider only the 30\n",
    "        most frequent cast / crew members / etc in the dataframe\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.variable = variable\n",
    "        self.top_number = top_number\n",
    "        \n",
    "        # Creates a list with each observation from that variable\n",
    "        self._list_of_obs = list(df[variable].apply(lambda x: [i['name'] for i in x] if x != {} else []).values)\n",
    "        \n",
    "        # Counts the number of occurrences for the top \"top_number\" cast / crew members on the df,\n",
    "        # (dictionary-like list, with tuples containing the names followed by their counter)\n",
    "        self.top_variable = Counter([i for j in self._list_of_obs for i in j]).most_common(top_number)\n",
    "        \n",
    "        # Grab only the cast / crew names, without their occurrences counter\n",
    "        self.top_variable_names = [x[0] for x in self.top_variable]\n",
    "\n",
    "        \n",
    "        return None\n",
    "\n",
    "    \n",
    "    def method(self, select):\n",
    "        \"\"\" Selects whether to call \"generate_counter_var\" or \"generate_dummies\"\n",
    "        \n",
    "        select: selected method name (counter, dummy)\n",
    "        \"\"\"\n",
    "        if (select != \"counter\") and (select != \"dummy\"):\n",
    "            raise ValueError(\"Error. Selection variable must be either 'counter' or 'dummy'\")\n",
    "        \n",
    "        # Getting rid of \"self\"\n",
    "        variable = self.variable\n",
    "        top_number = self.top_number\n",
    "        \n",
    "        # Creates new df to add the brand new variable\n",
    "        new_df = self.df\n",
    "        \n",
    "        # Creates a new string variable containing all the crew / cast members on df\n",
    "        new_df[variable + '_all'] = new_df[variable].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "        \n",
    "        \n",
    "        # Selection\n",
    "        if select == \"counter\":\n",
    "            new_df = self.generate_counter_var(variable, top_number, new_df)\n",
    "        else: # ie, if select == \"dummy\"\n",
    "            new_df = self.generate_dummies(variable, top_number, new_df)\n",
    "            \n",
    "\n",
    "        # Removes support variables created\n",
    "        new_df.drop([variable + '_all'], axis = 1, inplace = True)\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    \n",
    "    def generate_counter_var(self, variable, top_number, new_df):\n",
    "        \"\"\" Adds a variable to the df counting how many \"top_number\" cast / crew members are there on each movie. \n",
    "        \n",
    "        new_df: copy from the original df\n",
    "        \"\"\"\n",
    "    \n",
    "        def occurrence_counter(df_variable, list_of_names):\n",
    "            \"\"\" Counts number of famous cast / crew members on each movie \"\"\"\n",
    "            occurrences = 0\n",
    "            \n",
    "            for person in list_of_names:\n",
    "                if person in df_variable:\n",
    "                    occurrences += 1\n",
    "                    \n",
    "            return occurrences\n",
    "        \n",
    "        # Applies the previously defined function\n",
    "        new_df[variable + '_top_' + str(top_number) + '_counter'] = 0\n",
    "        new_df[variable + '_top_' + str(top_number) + '_counter'] = new_df[variable + '_all'].apply(lambda x: occurrence_counter(x, self.top_variable_names))\n",
    "        \n",
    "        return new_df\n",
    "    \n",
    "    \n",
    "    def generate_dummies(self, variable, top_number, new_df):\n",
    "        \"\"\" Creates dummies taking account if the movie belongs to the \"top_number\"\n",
    "        genra / or was developed by the \"top_number\" company\n",
    "        \"\"\"\n",
    "        # Creates dummy variables\n",
    "        for entry in self.top_variable_names:\n",
    "            new_df[variable + '_' + entry] = complete_df[variable + '_all'].apply(lambda x: 1 if entry in x else 0)\n",
    "            \n",
    "        return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Size\n",
    "for variable in ['crew', 'cast']:\n",
    "    complete_df[variable + '_size'] = complete_df[variable].apply(lambda x: len(x))\n",
    "\n",
    "# Top_50 dummies\n",
    "for variable in ['cast', 'crew']:\n",
    "    my_object = json_variables(complete_df, variable, 50)\n",
    "    complete_df = my_object.method(select = \"counter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Production Countries, production companies and genra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all these three variables, we will create variables counting their occurrences. For production companies, we'll also create dummy variables for the top 10 most common occurrences. For production countries, we'll do something different, however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming \"production\" to \"prod\", for the sake of simplicity\n",
    "complete_df.rename(columns={\"production_countries\": \"prod_countries\", \"production_companies\": \"prod_companies\"}, inplace = True)\n",
    "\n",
    "# Counter\n",
    "for variable in ['prod_countries', 'prod_companies', 'genres']:\n",
    "    complete_df[variable + '_count'] = complete_df[variable].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production companies and genres dummies\n",
    "for variable in ['prod_companies', 'genres']:\n",
    "    if variable == 'prod_companies':\n",
    "        x = 11\n",
    "    else:\n",
    "        x = 10\n",
    "    my_object = json_variables(complete_df, variable, x)\n",
    "    complete_df = my_object.method(select = \"dummy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all movies from \"Columbia Pictures Corporation\" are also counted as \"Columbia Pictures\" movies, we'll drop the first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all movies from \"Columbia Pictures Corporation\" are also counted as \"Columbia Pictures\" movies, we'll drop the first.\n",
    "complete_df.drop(['prod_companies_Columbia Pictures Corporation'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since more than half of the movies was produced in the United States, we'll create a dummy for it. Also, we'll create a dummy for being produced on at least one of the other top 9 most common countries. Note that a movie may be produced in more than one country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_object = json_variables(complete_df, 'prod_countries', 10)\n",
    "top_10_countries = my_object.top_variable_names\n",
    "\n",
    "# United States dummy\n",
    "complete_df['prod_countries_all'] = complete_df['prod_countries'].apply(lambda x: ' '.join(sorted([i['name'] for i in x])) if x != {} else '')\n",
    "complete_df['prod_countries_USA'] = complete_df['prod_countries_all'].apply(lambda x: 1 if 'United States of America' in x else 0)\n",
    "\n",
    "\n",
    "def produced_on_other_top_10(x, top_10_countries):\n",
    "    \"\"\" Returns 1 if the movie was produced in at least one of the other top 10\n",
    "    most common countries, and 0 otherwise\n",
    "    \"\"\"\n",
    "    # Excludes USA from the list\n",
    "    other_top_10_countries = top_10_countries[1:]\n",
    "    \n",
    "    for country in other_top_10_countries:\n",
    "        if country in x:\n",
    "            return 1\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    return 0\n",
    "\n",
    "\n",
    "# Other top 10 countries dummy:\n",
    "complete_df['prod_countries_other_top_10'] = complete_df['prod_countries_all'].apply(lambda x: produced_on_other_top_10(x, top_10_countries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homepage variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that about two-thirds of the movies have no homepage. Perhaps,this may be itself an useful information. Let's check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a variable for having a homepage\n",
    "complete_df['has_homepage'] = 0\n",
    "complete_df.loc[complete_df['homepage'].isnull() == False, 'has_homepage'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['has_homepage'].value_counts() # Counts the occurrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Belongs to collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the homepage information, I created a dummy for whether the movie belongs to a collection or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['has_collection'] = complete_df['belongs_to_collection'].apply(lambda x: int(0) if x != x else int(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original Language and spoken languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, since about 86% of the movies are in english, we'll create a dummy \"original_lang_en\" which takes 1 if the original language is english, and 0 elsewise. Also, we'll discard the \"spoken languages\" variable, as it probably is strongly correlated to the previous variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['original_lang_en'] = complete_df['original_language'].apply(lambda x: int(1) if x == \"en\" else int(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Status variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but not least, for some reason, there are four movies labeled as \"rumored\" or in \"post production\" (*ie*, not yet released). \n",
    "\n",
    "In spite of this, all of them have revenues values. Furthermore, when checking on the internet, one will find that all of them have been released, indeed. Hence, as only a minimal fraction of the movies are labeled as something other than \"released\", it is reasonably safe to drop this feature without loss of generality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['status'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[complete_df['status'] != 'Released'][['title','status','release_date','revenue']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data = complete_df, x = \"year\")\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=\"year\", y=\"ln_revenue\", data=complete_df)\n",
    "plt.xticks(rotation=90)\n",
    "sns.despine() # Removes some of the borders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is visible the increase in the number of movies along the years. The apparent decrease in 2018 is due to the lack of movies in the dataset after 2018-08-01. Also, the movies' revenue shows increasing volatily along the years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data = complete_df, x = \"release_weekday\").set(xlabel = \"Release weekday (0 == Monday)\", ylabel = \"Frequency\")\n",
    "\n",
    "# Second Plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=\"release_weekday\", y=\"ln_revenue\", data=complete_df)\n",
    "sns.despine() # Removes some of the borders\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is visible that the majority of movies has been released on a thursday or a friday. Furthermore, movies released on wednesday apparently have higher revenues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the movies are released on fridays. Also, the least profitable release days seems to be sunday, monday and saturday"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Release quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data = complete_df, x = \"release_quarter\").set(xlabel= \"Release quarter\", ylabel = \"Frequency\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=\"release_quarter\", y=\"ln_revenue\", data=complete_df).set(xlabel= \"Release quarter\", ylabel = \"Log-revenue\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most profitable quarter seems to be the last one, which is intuitive. However, surprisingly, the quarter with the most releases in the dataset is the third. Indeed, september was the month with the most releases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Budget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-budget *versus* log-revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.regplot(x=\"ln_budget\", y=\"ln_revenue\", data=complete_df)\n",
    "ax.set_title(\"Log-budget versus log-revenue\", fontsize = 15)\n",
    "ax.set_ylabel('Log-revenue')\n",
    "ax.set_xlabel('Log-budget')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The higher the budget, the higher the revenue, apparently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Popularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data = complete_df, x='popularity')\n",
    "plt.ylabel(\"Absolute frequency\")\n",
    "plt.xlabel(\"Popularity\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(data = complete_df, x='ln_popularity')\n",
    "plt.xlabel(\"Log-popularity\")\n",
    "plt.ylabel(\"Absolute frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Runtime variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.histplot(data = complete_df, x = \"runtime\").set(title='Runtime variable histogram', xlabel='Runtime', ylabel='Count')\n",
    "sns.despine() # Removes some of the borders\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(data = complete_df, x='runtime', y='ln_revenue')\n",
    "plt.title(\"Runtime vs. revenue\")\n",
    "plt.ylabel(\"Log-revenue\")\n",
    "plt.xlabel(\"Runtime\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.scatterplot(x=\"budget_runtime_ratio\", y=\"ln_revenue\", data=complete_df)\n",
    "ax.set_title(\"Budget to runtime ratio vs. log-revenue\", fontsize = 15)\n",
    "ax.set_ylabel('Revenue')\n",
    "ax.set_xlabel('Budget to runtime ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the movies are about 100 minutes long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Original language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(data = complete_df.loc[complete_df['original_language'].isin(complete_df['original_language'].value_counts().head(10).index)], x = \"original_language\", bins = 5)\n",
    "plt.title('Language count')\n",
    "plt.xlabel(\"Original languages\")\n",
    "\n",
    "# Second plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='original_lang_en', y='ln_revenue', data=complete_df)\n",
    "plt.title('Revenue per language')\n",
    "plt.ylabel(\"Log-revenue\")\n",
    "plt.xlabel(\"Is english the movie's original language?\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movies in english reach higher revenues, and english is also, by far, the most common language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cast and crew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
    "ax1.hist(complete_df['cast_size'])\n",
    "ax1.set_title('Cast size histogram')\n",
    "ax2.hist(complete_df['crew_size'])\n",
    "ax2.set_title('Crew size histogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of famous crew / cast members per movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['crew_top_50_counter'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['cast_top_50_counter'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list of movies with large number of famous cast includes Pulp Fiction, Megamind and Good Will Hunting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.loc[complete_df['cast_top_50_counter'] >= 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Production countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x='prod_countries_USA', y='ln_revenue', data=complete_df)\n",
    "plt.title('Log-revenue for USA x non-USA prod. countries')\n",
    "plt.ylabel(\"Log-revenue\")\n",
    "plt.xlabel(\"Prod. country == USA\")\n",
    "sns.despine() # Removes some of the borders\n",
    "\n",
    "# Second plot\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x='prod_countries_other_top_10', y='ln_revenue', data=complete_df)\n",
    "plt.title('Log-revenue for other top 10 most common prod. countries')\n",
    "plt.ylabel(\"Log-revenue\")\n",
    "plt.xlabel(\"Prod. country belongs to the other top 10 most frequent (USA incl.)?\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='prod_countries_other_top_10', y='ln_revenue', data=complete_df.loc[complete_df['prod_countries_USA'] == 0])\n",
    "plt.title('Revenue for other top 10 most common prod. countries')\n",
    "plt.ylabel(\"Log-revenue\")\n",
    "plt.xlabel(\"Prod. country belongs to the other top 10 most frequent (USA excl.)?\")\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movies produced in the United States show greater revenues than their counterparts. Movies made on the other top 10 countries have almost similar performance to those made outside them. However, when excluding the 'made in USA' films from the comparison, the other top 10 countries perfom better than their counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Homepage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='has_homepage', y='ln_revenue', data=complete_df)\n",
    "plt.title('Revenue comparison for movies with and without homepages')\n",
    "plt.ylabel(\"Log-revenue\")\n",
    "plt.xlabel(\"Has homepage?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the data, movies with homepages presents slightly higher revenues in comparison with those without website pages. However, the website itself probably is not a useful piece of information; therefore, this piece of information will be dropped later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Belongs to collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='has_collection', y='ln_revenue', data=complete_df)\n",
    "plt.ylabel(\"Log-revenue\")\n",
    "plt.xlabel(\"Movie belongs to collection?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movies that are part of franchises shows higher revenues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummies for date variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = pd.get_dummies(complete_df, columns=['release_quarter'], drop_first = True)\n",
    "complete_df['release_weekday_friday'] = complete_df['release_weekday'].apply(lambda x: 1 if x == 4 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping useless variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since \"original_lang_en\" and \"prod_countries_USA\" are highly correlated, we'll drop the former."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drops useless variables\n",
    "df = complete_df.drop(['id','imdb_id', 'original_title', 'original_language', 'status', 'poster_path',\n",
    "                  'belongs_to_collection', 'homepage', 'spoken_languages', 'tagline', 'overview',\n",
    "                  'prod_countries', 'prod_companies', 'crew', 'cast', 'genres',\n",
    "                  'belongs_to_collection', 'budget', 'Keywords', 'title', 'popularity','prod_countries_all',\n",
    "                  'release_date', 'genres_count', 'original_lang_en', 'release_weekday', 'revenue'],\n",
    "                 axis = 1) # Axis = 1 means you're dropping a COLUMN, not a row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('df.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "df_without_release_date = df.drop(['release_weekday_2', 'release_weekday_2','release_weekday_3', 'release_weekday_4',\n",
    "                                    'release_weekday_5', 'release_weekday_6', 'release_month_2', 'release_month_3', \n",
    "                                    'release_month_4', 'release_month_5', 'release_month_6', 'release_month_7', \n",
    "                                    'release_month_8', 'release_month_9', 'release_month_10', 'release_month_11', \n",
    "                                    'release_month_12'], axis = 1) \n",
    "\n",
    "corr = df_without_release_date.corr()\n",
    "sns.heatmap(corr, annot = True, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Divida os dados em treino/teste: Sua base é balanceada? Você precisa pensar em alguma estratégia especial para dividir a sua base? Você considera estratégias como *K-fold validation*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "# Dimensionality reduction\n",
    "from sklearn.preprocessing import StandardScaler # To use on X (features) before the PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Model selection\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data stratifying by \"prod country == USA\"\n",
    "df_train, df_test = model_selection.train_test_split(df, test_size = 0.3, random_state = 0, stratify = df['prod_countries_USA'])\n",
    "print(df_train['prod_countries_USA'].mean())\n",
    "print(df_test['prod_countries_USA'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Benchmark model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4)** Construa o seu *benchmark*. Sempre comece com modelo bem simples que você tenha controle. Uma boa escolha é um modelo linear em que  você consiga interpretar os seus resultados/ter uma ideia das variáveis que podem influenciar os seus resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySklearningModel:\n",
    "    \"\"\" Object for the regression models. Accepts the use of PCA beforehand, and also\n",
    "    has a method for running cross-validation\n",
    "    \n",
    "    TO BE IMPLEMENTED:\n",
    "    - Graphs for the cross-validation comparisons\n",
    "    - Method for printing fitted coefficients\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, df_train, df_test, independent_variable_list, dependent_variable, \n",
    "                 use_pca = False, pca_variance = 0.95):\n",
    "        \"\"\" Initiates\n",
    "        \n",
    "        model: model's type\n",
    "        df_train, df_test: train and test dataframes\n",
    "        independent_variable_list: features (X variables)\n",
    "        dependent_variable: target (y variable)\n",
    "        use_pca: whether to use or not PCA for dimensionality reduction (default: false)\n",
    "        pca_variance: explained variance threshold for the PCA. Goes from zero (0%) to one (100%) (default: 0.95)\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.independent_variable_list = independent_variable_list\n",
    "        self.dependent_variable = dependent_variable\n",
    "        self.X_train, self.X_test = df_train[self.independent_variable_list].values, df_test[self.independent_variable_list].values\n",
    "        self.y_train, self.y_test = np.squeeze(df_train[[self.dependent_variable]].values), np.squeeze(df_test[[self.dependent_variable]].values)\n",
    "        self.pca_variance = pca_variance\n",
    "        \n",
    "        # Dimensionality Reduction\n",
    "        if use_pca == True:\n",
    "            # Rescaling before the PCA (necessary)\n",
    "            my_scaler = StandardScaler()\n",
    "            new_dataset = my_scaler.fit_transform(self.X_train) # Rescales\n",
    "            self.X_test = my_scaler.transform(self.X_test) # Rescales\n",
    "\n",
    "            pca = PCA(n_components = self.pca_variance)\n",
    "            self.X_train = pca.fit_transform(new_dataset)\n",
    "            self.X_test = pca.transform(self.X_test)\n",
    "            \n",
    "        return None\n",
    "\n",
    "    \n",
    "    def run_sklearn_regression_crossval(self, number_splits, score_list, random_state = 0):\n",
    "        \"\"\" Runs the regression (USE IT FOR CROSS-VALIDATION)\n",
    "        \n",
    "        number_splits: number of splits for the kfold\n",
    "        score_list: performance's metrics to be used\n",
    "        random_state: random seed (default = 0)\n",
    "        \"\"\"\n",
    "        kfold = model_selection.KFold(n_splits=number_splits, shuffle=True, random_state=random_state)\n",
    "        results = model_selection.cross_validate(self.model, self.X_train, self.y_train, cv=kfold, scoring=score_list,return_train_score=True)\n",
    "        \n",
    "        print(str(self.model))\n",
    "        \n",
    "        for score in score_list:\n",
    "            print(score+':')\n",
    "            print('Cross-val Train: '+'Mean',np.mean(results['train_'+score]),\n",
    "            'Standard Error',np.std(results['train_'+score]))\n",
    "            print('Cross-val Test: '+'Mean',np.mean(results['test_'+score]),\n",
    "            'Standard Error',np.std(results['test_'+score]))\n",
    "            print(\"\")\n",
    "            \n",
    "    def run_sklearn_regression(self, score_list):\n",
    "        \"\"\" Runs the regression \"\"\"\n",
    "        # Fits the model and predicts y\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        self.y_pred = self.model.predict(self.X_test)\n",
    "        \n",
    "        print(self.model)\n",
    "    \n",
    "        # Prints the performance metrics:\n",
    "        for score in score_list:\n",
    "            # If opted for MSE, returns RMSE\n",
    "            if score == \"mean_squared_error\":\n",
    "                print(\"root_\" + score + \": \" + str(eval(score + '(self.y_test, self.y_pred, squared = False)')))\n",
    "            else:\n",
    "                print(score + \": \" + str(eval(score + '(self.y_test, self.y_pred)')))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def prints_parameters(self):\n",
    "        \"\"\" [NOT WORKING PROPERLY] Prints the model's parameters\"\"\"\n",
    "        # Checks whether the model has already been fitted or not.\n",
    "        if hasattr(self.model, 'coef_'):\n",
    "            for i in range(len(self.independent_variable_list)):\n",
    "                print(self.independent_variable_list[i] + \": \" + str(self.model.coef_[i]))\n",
    "        else:\n",
    "            print(\"Model has not yet been fitted.\")\n",
    "        \n",
    "        return None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    number_splits=5\n",
    "    \n",
    "    dependent_variable = 'ln_revenue'\n",
    "    independent_variable_list = df.columns.values\n",
    "    independent_variable_list = [x for x in independent_variable_list if x != dependent_variable]\n",
    "    \n",
    "    for model in [LinearRegression(), KNeighborsRegressor(n_neighbors = 10),\n",
    "                  RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 0)]:\n",
    "        \n",
    "        my_model = MySklearningModel(model,df_train, df_test, independent_variable_list,dependent_variable, \n",
    "                                     use_pca = True, pca_variance = 0.99)\n",
    "        \n",
    "        # Cross-validation\n",
    "        my_model.run_sklearn_regression_crossval(number_splits, ['neg_mean_squared_error', 'r2'])\n",
    "        \n",
    "        # Regression\n",
    "        #my_model.run_sklearn_regression(['mean_squared_error', 'r2_score'])\n",
    "        \n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHECK IF BELOW CODE IS __REALLY__ RELEVANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code was used for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    number_splits=5\n",
    "    \n",
    "    dependent_variable = 'ln_revenue'\n",
    "    independent_variable_list = df.columns.values\n",
    "    independent_variable_list = [x for x in independent_variable_list if x != dependent_variable]\n",
    "    \n",
    "    for i in range(1, 15):\n",
    "        print(\"Number of neighbours: \", i)\n",
    "        \n",
    "        model = KNeighborsRegressor(n_neighbors = i)\n",
    "        my_model = MySklearningModel(model,df_train, df_test,\n",
    "        independent_variable_list,dependent_variable)\n",
    "        \n",
    "        my_model.run_sklearn_regression_crossval(number_splits,\n",
    "        ['neg_mean_squared_error', 'r2'])\n",
    "        #my_model.run_sklearn_regression(['mean_squared_error', 'r2_score'])\n",
    "        #print(\"\\nParameters:\")\n",
    "        #my_model.prints_parameters()\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross-validation, I opted to run the KNeighborsRegressor using n = 10 neighbors. After this threshold, the gains on the tests' scores were marginal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6)** Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6)** Você precisará de um procedimento específico de seleção de variáveis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    dependent_variable = 'ln_revenue'\n",
    "        \n",
    "    independent_variable_list = df.columns.values\n",
    "    independent_variable_list = [x for x in independent_variable_list if x != dependent_variable]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    X = df.drop([dependent_variable], axis = 1) # Features\n",
    "    #X = df[independent_variable_list].values\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    print(X)\n",
    "    y = df[dependent_variable] # Target variable\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d = np.argmax(cumsum >= 0.95) + 1\n",
    "    print(d)\n",
    "\n",
    "    # Another way around:        \n",
    "    pca = PCA(n_components=0.95)\n",
    "    projected2again = pca.fit_transform(X)\n",
    "    \n",
    "    # Yet another way:\n",
    "    plt.figure(0)\n",
    "    plt.plot(cumsum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(projected2again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    dependent_variable = 'ln_revenue'\n",
    "    \n",
    "    X = df.drop([dependent_variable], axis = 1) # Features\n",
    "    y = df[dependent_variable] # Target variable\n",
    "    \n",
    "    independent_variable_list = df.columns.values\n",
    "    independent_variable_list = [x for x in independent_variable_list if x != dependent_variable]\n",
    "    \n",
    "    \"\"\"\n",
    "    pca = PCA(1)\n",
    "    pca.fit(X)\n",
    "    projected2again = pca.fit_transform(X)    \n",
    "\n",
    "    my_projection_X = []\n",
    "    my_projection_y = []\n",
    "\n",
    "    for i in range(len(projected2again)):\n",
    "        my_projection_X.append(projected2again[i][0])\n",
    "\n",
    "    my_projection_y = [value for value in y.values]\n",
    "\n",
    "    d = {'y':my_projection_y,'X':my_projection_X}\n",
    "    df_post_pca = pd.DataFrame(d)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_components = 15\n",
    "    pca = PCA(n_components)\n",
    "    df_post_pca = pd.DataFrame(pca.fit_transform(X), columns=['PCA%i' % i for i in range(n_components)], index=X.index)\n",
    "    df_post_pca['y'] = df[dependent_variable]\n",
    "\n",
    "    \n",
    "    \n",
    "    number_splits=5\n",
    "    \n",
    "    dependent_variable = 'y'\n",
    "    independent_variable_list = df_post_pca.columns.values\n",
    "    independent_variable_list = [x for x in independent_variable_list if x != dependent_variable]\n",
    "    \n",
    "    for model in [LinearRegression(), KNeighborsRegressor(n_neighbors = 10),\n",
    "                  RandomForestRegressor(n_estimators = 100, max_depth = 10, random_state = 0)]:\n",
    "        my_model = MySklearningModel(model,df_post_pca,\n",
    "        independent_variable_list,dependent_variable)\n",
    "        \n",
    "        print(str(model) + \":\")\n",
    "        my_model.run_sklearn_regression_crossval(number_splits,\n",
    "        ['neg_mean_squared_error', 'r2'])\n",
    "        print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
